{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_PROJECT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRMDxi4pLl0L",
        "outputId": "cfa0a1ee-c276-40d7-b8e4-7a04870e69b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QTIWNvQMIQe",
        "outputId": "f078724c-8eb6-4bb9-fd95-5442e497f11c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "test=pd.read_csv(root_path+'/Sarcasm_Headlines_Dataset2.csv',header = 0)\n",
        "type(test)\n",
        "test.head()\n",
        "test.describe\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of                                                 headline  ...  tri-gram\n",
              "0      former versace store clerk sues over secret 'b...  ...         0\n",
              "1      the 'roseanne' revival catches up to our thorn...  ...         0\n",
              "2      mom starting to fear son's web series closest ...  ...         0\n",
              "3      boehner just wants wife to listen, not come up...  ...         0\n",
              "4      j.k. rowling wishes snape happy birthday in th...  ...         0\n",
              "...                                                  ...  ...       ...\n",
              "26704               american politics in moral free-fall  ...         0\n",
              "26705                            america's best 20 hikes  ...         0\n",
              "26706                              reparations and obama  ...         0\n",
              "26707  israeli ban targeting boycott supporters raise...  ...         0\n",
              "26708                  gourmet gifts for the foodie 2014  ...         0\n",
              "\n",
              "[26709 rows x 19 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdyytrnC1nWX",
        "outputId": "6f26d263-b187-4252-b370-281879dbcf5c"
      },
      "source": [
        "pip install senticnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting senticnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/8e/c72d2c5186f762f13d3d8cf63aedeb973b95d45d62b59a63006060073e3a/senticnet-1.6-py3-none-any.whl (51.9MB)\n",
            "\u001b[K     |████████████████████████████████| 51.9MB 57kB/s \n",
            "\u001b[?25hInstalling collected packages: senticnet\n",
            "Successfully installed senticnet-1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS9IsJhmUwvP",
        "outputId": "f7fd9aef-df06-4b14-976c-c720ddd5380a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NusguGbSFly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf907d1-6d93-4bc1-91c9-117353e4b68f"
      },
      "source": [
        "# Contradiction in the sentiment score and positive neagtive scores of headlines using senticnet\n",
        "from senticnet.senticnet import SenticNet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sn = SenticNet()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "contra=[]\n",
        "neg=[]\n",
        "pos=[]\n",
        "low_pos=[]\n",
        "med_pos=[]\n",
        "high_pos=[]\n",
        "low_neg=[]\n",
        "med_neg=[]\n",
        "high_neg=[]\n",
        "headlines=test['headline'].values\n",
        "\n",
        "\n",
        "for i in headlines:\n",
        "  temp_neg=0\n",
        "  temp_pos=0\n",
        "  a=i.split()\n",
        "  for j in range(len(a)):\n",
        "    a[j]=lemmatizer.lemmatize(a[j])\n",
        "  \n",
        "  for i in range(len(a)):\n",
        "    try:\n",
        "      concept_info = sn.concept(a[i])\n",
        "      if (concept_info['polarity_label']=='positive'):\n",
        "        temp_pos=temp_pos + float (concept_info['polarity_value'])\n",
        "      if (concept_info['polarity_label']=='negative'):\n",
        "        temp_neg=temp_neg + float (concept_info['polarity_value'])\n",
        "    except KeyError:\n",
        "      temp_neg=temp_neg+0\n",
        "      temp_pos=temp_pos+0\n",
        "\n",
        "  temp_neg=temp_neg*-1\n",
        "  if (temp_pos < 1 and temp_pos >=0):\n",
        "    low_pos.append(1)\n",
        "    med_pos.append(0)\n",
        "    high_pos.append(0)\n",
        "  if (temp_pos >= 1 and temp_pos < 2):\n",
        "    low_pos.append(0)\n",
        "    med_pos.append(1)\n",
        "    high_pos.append(0)\n",
        "  if (temp_pos >=2):\n",
        "    low_pos.append(0)\n",
        "    med_pos.append(0)\n",
        "    high_pos.append(1) \n",
        "  if (temp_neg < 1 and temp_neg >=0):\n",
        "    low_neg.append(1)\n",
        "    med_neg.append(0)\n",
        "    high_neg.append(0)\n",
        "  if (temp_neg >= 1 and temp_neg < 2):\n",
        "    low_neg.append(0)\n",
        "    med_neg.append(1)\n",
        "    high_neg.append(0)\n",
        "  if (temp_neg >= 2):\n",
        "    low_neg.append(0)\n",
        "    med_neg.append(0)\n",
        "    high_neg.append(1) \n",
        "  if (temp_pos != 0 and temp_neg !=0):\n",
        "    contra.append(1)\n",
        "  else:\n",
        "    contra.append(0)    \n",
        "\n",
        "  neg.append(temp_neg)\n",
        "  pos.append(temp_pos)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-ClUzhfeQb5"
      },
      "source": [
        "test['sum_neg']=neg\n",
        "test['sum_pos']=pos\n",
        "test['neg_low']=low_neg\n",
        "test['neg_med']=med_neg\n",
        "test['neg_high']=high_neg\n",
        "test['pos_low']=low_pos\n",
        "test['pos_medium']=med_pos\n",
        "test['pos_high']=high_pos\n",
        "test['contra']=contra\n",
        "test.to_csv(root_path+'/Sarcasm_Headlines_Dataset2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKIIZRNJBxCp"
      },
      "source": [
        "#punctuations in the headlines\n",
        "\n",
        "\n",
        "import string\n",
        "\n",
        "punc_med=[]\n",
        "punc_low=[]\n",
        "punc_high=[]\n",
        "for j in headlines:\n",
        "  count=0\n",
        "  Sentence=j\n",
        "  for i in Sentence:\n",
        "    if i in string.punctuation:\n",
        "      count=count+1\n",
        "  if (count==0):\n",
        "    punc_low.append(1)\n",
        "    punc_med.append(0)\n",
        "    punc_high.append(0)\n",
        "  if (count >= 1 and count <=3):\n",
        "    punc_low.append(0)\n",
        "    punc_med.append(1)\n",
        "    punc_high.append(0)\n",
        "  if (count >=4):\n",
        "    punc_low.append(0)\n",
        "    punc_med.append(0)\n",
        "    punc_high.append(1)\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtwpCkQOFkPq"
      },
      "source": [
        "test['punct_low']=punc_low\n",
        "test['punc_med']=punc_med\n",
        "test['punc_high']=punc_high\n",
        "test.to_csv(root_path+'/Sarcasm_Headlines_Dataset2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hBavwb3GzXq"
      },
      "source": [
        "$#capital letters in headlines\n",
        "\n",
        "import re\n",
        "\n",
        "cap_med=[]\n",
        "cap_low=[]\n",
        "cap_high=[]\n",
        "\n",
        "\n",
        "for i in headlines:\n",
        "  count_capital=0\n",
        "  line = i\n",
        "  capital = re.findall(r'(\\b[A-Z]([a-z])*\\b)',line)\n",
        "  count_capital=len(capital)\n",
        "  if (count_capital==0):\n",
        "    cap_low.append(1)\n",
        "    cap_med.append(0)\n",
        "    cap_high.append(0)\n",
        "  if (count_capital >= 1 and count_capital <=3):\n",
        "    cap_low.append(0)\n",
        "    cap_med.append(1)\n",
        "    cap_high.append(0)\n",
        "  if (count_capital >=4):\n",
        "    cap_low.append(0)\n",
        "    cap_med.append(0)\n",
        "    cap_high.append(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMfNMQUSNPRZ"
      },
      "source": [
        "test['capital_low']=cap_low\n",
        "test['capital_med']=cap_med\n",
        "test['capital_high']=cap_high\n",
        "test.to_csv(root_path+'/Sarcasm_Headlines_Dataset2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4bqaBg2ORrL"
      },
      "source": [
        "#bigrams and trigrams in headlines\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk import bigrams\n",
        "from nltk import trigrams\n",
        "import collections\n",
        "\n",
        "tri=[]\n",
        "bi=[]\n",
        "for i in headlines:\n",
        "  res=0\n",
        "  res2=0\n",
        "  line=i\n",
        "  line=line.split()\n",
        "  big=bigrams(line)\n",
        "  big=list (big)\n",
        "  tig=trigrams(line)\n",
        "  tig=list (tig)\n",
        "  res=[item for item, count in collections.Counter(big).items() if count > 1]\n",
        "  res2=[item for item, count in collections.Counter(tig).items() if count > 1]\n",
        "  if (len(res) > 0):\n",
        "    bi.append(1)\n",
        "  elif (len(res)==0):\n",
        "    bi.append(0)  \n",
        "  if (len(res2) > 0):\n",
        "    tri.append(1)\n",
        "  elif (len(res2)==0):\n",
        "    tri.append(0)   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwm_AbIPZx-a"
      },
      "source": [
        "test['bi-gram']=bi\n",
        "test['tri-gram']=tri\n",
        "test.to_csv(root_path+'/Sarcasm_Headlines_Dataset2.csv',index=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKhKnNZEbhvS",
        "outputId": "7814fbe8-0958-4ae3-f0e9-c1dd17a4d0be"
      },
      "source": [
        "import numpy as np\n",
        "test.columns\n",
        "features=test[['sum_pos','sum_neg','contra','pos_low','pos_medium','pos_high','neg_low','neg_med','punct_low','punc_med','punc_high']]\n",
        "\n",
        "X=np.asarray(features)\n",
        "\n",
        "y=np.asarray(test['is_sarcastic'])\n",
        "\n",
        "X[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.875, 0.64 , 1.   , 1.   , 0.   , 0.   , 1.   , 0.   , 0.   ,\n",
              "        1.   , 0.   ],\n",
              "       [1.17 , 2.76 , 1.   , 0.   , 1.   , 0.   , 0.   , 0.   , 0.   ,\n",
              "        1.   , 0.   ],\n",
              "       [1.716, 0.66 , 1.   , 0.   , 1.   , 0.   , 1.   , 0.   , 0.   ,\n",
              "        1.   , 0.   ],\n",
              "       [0.797, 0.   , 0.   , 1.   , 0.   , 0.   , 1.   , 0.   , 0.   ,\n",
              "        1.   , 0.   ],\n",
              "       [1.901, 0.   , 0.   , 0.   , 1.   , 0.   , 1.   , 0.   , 0.   ,\n",
              "        1.   , 0.   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83-55c1GdjhX",
        "outputId": "84ac66b9-3d59-4c2b-c5e1-202d615b778b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=41)\n",
        "\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18696,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq6FJnaGeThh"
      },
      "source": [
        "from sklearn import svm\n",
        "classifier=svm.SVC(kernel='linear',gamma='auto',C=2)\n",
        "classifier.fit(X_train,y_train)\n",
        "y_predict=classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPl3ZObyfJYX",
        "outputId": "65ba5452-f7ff-48e5-cb98-5b7f8cb1be89"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn import metrics\n",
        "print(classification_report(y_test,y_predict))\n",
        "print(metrics.confusion_matrix(y_test, y_predict))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63      4488\n",
            "           1       0.52      0.49      0.51      3525\n",
            "\n",
            "    accuracy                           0.58      8013\n",
            "   macro avg       0.57      0.57      0.57      8013\n",
            "weighted avg       0.57      0.58      0.57      8013\n",
            "\n",
            "[[2878 1610]\n",
            " [1790 1735]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRPtqi51VgDU",
        "outputId": "89ea6c84-68e0-4a5f-ce39-be4f36f01a9d"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_predictNB=clf.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,y_predictNB))\n",
        "print(metrics.confusion_matrix(y_test, y_predictNB))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.78      0.68      4488\n",
            "           1       0.56      0.37      0.45      3525\n",
            "\n",
            "    accuracy                           0.60      8013\n",
            "   macro avg       0.59      0.57      0.56      8013\n",
            "weighted avg       0.59      0.60      0.58      8013\n",
            "\n",
            "[[3484 1004]\n",
            " [2228 1297]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZNPX5qlDs1S",
        "outputId": "d5ae4eac-3d4c-4ac5-e818-604162ca2b74"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "lsvc = LinearSVC()\n",
        "lsvc.fit(X_train,y_train)\n",
        "y_predictLSVC=lsvc.predict(X_test)\n",
        "print(classification_report(y_test,y_predictLSVC))\n",
        "print(metrics.confusion_matrix(y_test, y_predictLSVC))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.79      0.69      4488\n",
            "           1       0.57      0.36      0.44      3525\n",
            "\n",
            "    accuracy                           0.60      8013\n",
            "   macro avg       0.59      0.57      0.57      8013\n",
            "weighted avg       0.59      0.60      0.58      8013\n",
            "\n",
            "[[3533  955]\n",
            " [2250 1275]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJtmx-GkewnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa87646b-e15a-497e-a834-c7a8b3ede4e8"
      },
      "source": [
        "\n",
        "# predicting new headline as sarcastic or not\n",
        "\n",
        "user_sum_pos=0\n",
        "user_sum_neg=0\n",
        "user_pos_low=0\n",
        "user_contra=0\n",
        "user_pos_medium=0\n",
        "user_pos_high=0\n",
        "user_neg_low=0\n",
        "user_neg_med=0\n",
        "user_punct_low=0\n",
        "user_punc_med=0\n",
        "user_punc_high=0\n",
        "\n",
        "a=input ()\n",
        "b=a\n",
        "a=a.split()\n",
        "\n",
        "for j in range(len(a)):\n",
        "  \n",
        "  a[j]=lemmatizer.lemmatize(a[j])\n",
        "for i in range(len(a)):\n",
        "  try:\n",
        "    concept_info = sn.concept(a[i])\n",
        "    if (concept_info['polarity_label']=='positive'):\n",
        "      temp_pos=temp_pos + float (concept_info['polarity_value'])\n",
        "    if (concept_info['polarity_label']=='negative'):\n",
        "      temp_neg=temp_neg + float (concept_info['polarity_value'])\n",
        "  except KeyError:\n",
        "    temp_neg=temp_neg+0\n",
        "    temp_pos=temp_pos+0\n",
        "\n",
        "temp_neg=temp_neg*-1\n",
        "if (temp_pos < 1 and temp_pos >=0):\n",
        "  user_pos_low=1\n",
        "  user_pos_medium=0\n",
        "  user_pos_high=0\n",
        "if (temp_pos >= 1 and temp_pos < 2):\n",
        "  user_pos_low=0\n",
        "  user_pos_medium=1\n",
        "  user_pos_high=0\n",
        "if (temp_pos >=2):\n",
        "  user_pos_low=0\n",
        "  user_pos_medium=0\n",
        "  user_pos_high=1\n",
        "if (temp_neg < 1 and temp_neg >=0):\n",
        "  user_neg_low=1\n",
        "  user_neg_medium=0\n",
        "  user_neg_high=0\n",
        "if (temp_neg >= 1 and temp_neg < 2):\n",
        "  user_neg_low=0\n",
        "  user_neg_medium=1\n",
        "  user_neg_high=0\n",
        "if (temp_neg >= 2):\n",
        "  user_neg_low=0\n",
        "  user_neg_medium=0\n",
        "  user_neg_high=1\n",
        "if (temp_pos != 0 and temp_neg !=0):\n",
        "  user_contra=1\n",
        "else:\n",
        "  user_contra=0 \n",
        "user_sum_pos=temp_pos\n",
        "user_sum_neg=temp_neg\n",
        "\n",
        "\n",
        "count=0\n",
        "Sentence=b\n",
        "for i in Sentence:\n",
        "  if i in string.punctuation:\n",
        "    count=count+1\n",
        "if (count==0):\n",
        "  user_punct_low=1\n",
        "  user_punc_med=0\n",
        "  user_punc_high=0\n",
        "if (count >= 1 and count <=3):\n",
        "  user_punct_low=0\n",
        "  user_punc_med=1\n",
        "  user_punc_high=0\n",
        "if (count >=4):\n",
        "  user_punct_low=0\n",
        "  user_punc_med=0\n",
        "  user_punc_high=1  \n",
        "\n",
        "user_test=[[user_sum_pos,user_sum_neg,user_contra,user_pos_low,user_pos_medium,user_pos_high,user_neg_low,user_neg_med,user_punct_low,user_punc_med,user_punc_high]]\n",
        "user_predict=classifier.predict(user_test)\n",
        "print(user_predict)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he is a killer\n",
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}